# Human Evaluation Rubric for Generated Prompts

Rate each generated prompt (1 = poor, 5 = excellent) on the following axes:

1. **Elicitation (1-5)**: Does the prompt invite the student to explain their reasoning?
2. **Non-spoiling (1-5)**: Does the prompt avoid giving away the solution or key lines of code?
3. **Targeting (1-5)**: Does the prompt address the likely misconception or the area the student erred on?
4. **Actionability (1-5)**: Does the prompt suggest a small, concrete next step (test, thought experiment)?
5. **Overall usefulness (1-5)**: Would this prompt help a typical learner deepen understanding?

Collect ratings from at least 3 instructors/TAs per sample and compute mean scores and inter-rater agreement (Cohen's kappa or Krippendorff's alpha).
